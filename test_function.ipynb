{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"143hNnKcKfn0iSQNHriGrnVncAyTtlj7s","timestamp":1605033891947}],"toc_visible":true,"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"QcJK3kXl--c3"},"source":["# Computer Vision Coursework Submission (INM460)\n","\n","**Student name, ID and cohort:** Nicolò Finzi (240041165) - PG\n"]},{"cell_type":"markdown","metadata":{"id":"6rVDkKZEEKIp"},"source":["# Notebook Setup\n","In this section you should include all the code cells required to test your coursework submission. Specifically:"]},{"cell_type":"markdown","source":["### Mount Google Drive"],"metadata":{"id":"I3D_ONirhSvw"}},{"cell_type":"code","metadata":{"id":"Kl3ZyAAVEKI1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745760616085,"user_tz":-60,"elapsed":18236,"user":{"displayName":"Nicolo Finzi","userId":"07961935095970494315"}},"outputId":"9323f37e-53d1-4bbb-bed2-8a1fae21d4dd"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"RxIKS3ukEKI2"},"source":["### Define Local Path\n","\n","In the next cell you should assign to the variable `GOOGLE_DRIVE_PATH_AFTER_MYDRIVE` the relative path of this folder in your Google Drive.\n","\n","**IMPORTANT:** you have to make sure that **all the files required to test your functions are loaded using this variable** (as was the case for all lab tutorials). In other words, do not use in the notebook any absolute paths. This will ensure that the markers can run your functions. Also, **do not use** the magic command `%cd` to change directory.\n","\n"]},{"cell_type":"code","metadata":{"id":"28VgE7dMEKI2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745760621837,"user_tz":-60,"elapsed":578,"user":{"displayName":"Nicolo Finzi","userId":"07961935095970494315"}},"outputId":"34fe4ee7-8d82-4378-a6b7-338a4523426c"},"source":["import os\n","\n","# TODO: Fill in the Google Drive path where you uploaded the CW_folder_PG\n","# Example: GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'Colab Notebooks/Computer Vision/CW_folder_PG'\n","\n","GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'Computer Vision/CW_Folder_PG'\n","GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n","print(os.listdir(GOOGLE_DRIVE_PATH))"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["['Code', 'CW_Dataset', 'Models', 'Personal_Video', 'test_function.ipynb']\n"]}]},{"cell_type":"markdown","metadata":{"id":"747B4GKgvHsv"},"source":["### Load packages\n","\n","In the next cell you should load all the packages required to test your function."]},{"cell_type":"code","source":["!pip install mtcnn"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e70PgOxJrwB6","executionInfo":{"status":"ok","timestamp":1745760626748,"user_tz":-60,"elapsed":2933,"user":{"displayName":"Nicolo Finzi","userId":"07961935095970494315"}},"outputId":"d057488d-0d2d-4e39-d070-55728970fe9b"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting mtcnn\n","  Downloading mtcnn-1.0.0-py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: joblib>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from mtcnn) (1.4.2)\n","Collecting lz4>=4.3.3 (from mtcnn)\n","  Downloading lz4-4.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n","Downloading mtcnn-1.0.0-py3-none-any.whl (1.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lz4-4.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: lz4, mtcnn\n","Successfully installed lz4-4.4.4 mtcnn-1.0.0\n"]}]},{"cell_type":"code","source":["!pip install lz4"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SrIrrDdStHhb","executionInfo":{"status":"ok","timestamp":1745760631999,"user_tz":-60,"elapsed":5251,"user":{"displayName":"Nicolo Finzi","userId":"07961935095970494315"}},"outputId":"51f6c1b6-6365-41ae-da18-87a409652d3f"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: lz4 in /usr/local/lib/python3.11/dist-packages (4.4.4)\n"]}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import numpy as np\n","from joblib import dump, load\n","import torch\n","import torch.nn as nn\n","from torchvision import models\n","import cv2\n","from matplotlib import rc\n","import matplotlib.animation as animation\n","from mtcnn import MTCNN\n","from matplotlib import patches\n","import torchvision.transforms as transforms\n","import torchvision\n","from PIL import Image"],"metadata":{"id":"FlNicHnRkrcp","executionInfo":{"status":"ok","timestamp":1745760652711,"user_tz":-60,"elapsed":20711,"user":{"displayName":"Nicolo Finzi","userId":"07961935095970494315"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["### Load models\n","\n","In the next cell you should load your best performing model (this might consist of more than one file). Avoid to load it within `MaskDetection` to avoid having to reload it each time."],"metadata":{"id":"O_ag6odkk1Ki"}},{"cell_type":"code","source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Assuming that we are on a CUDA machine, this should print a CUDA device:\n","print(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qADmUwM0U4Ii","executionInfo":{"status":"ok","timestamp":1745760652735,"user_tz":-60,"elapsed":16,"user":{"displayName":"Nicolo Finzi","userId":"07961935095970494315"}},"outputId":"82b8c2bb-030f-4c35-a9b0-df860db0f965"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda:0\n"]}]},{"cell_type":"code","source":["model_ft = models.resnet18(weights='IMAGENET1K_V1')\n","num_ftrs = model_ft.fc.in_features\n","model_ft.fc = nn.Linear(num_ftrs, 3)  # 3 output classes\n","                                      # (no mask, correct mask, incorrect mask)\n","\n","model_ft = model_ft.to(device)\n","\n","CNN = torch.load(os.path.join(GOOGLE_DRIVE_PATH, 'Models','BEST_CNN.pth'))\n","\n","model_ft.load_state_dict(CNN)"],"metadata":{"id":"UCpF4Aa6lhnO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745760657237,"user_tz":-60,"elapsed":4508,"user":{"displayName":"Nicolo Finzi","userId":"07961935095970494315"}},"outputId":"bcc6d8df-09ad-4b64-cc89-eb1cfb16f1d9"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 185MB/s]\n"]},{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"Qc83ETI1a3o9"},"source":["# Test MaskDetection\n","\n","This section should allow to test the `MaskDetection` function. First, add cells with the code needed to load the necessary subroutines to make `MaskDetection` work."]},{"cell_type":"code","source":["def MaskDetection(path_to_test):\n","  # The following code section was taken from lab 4 [1]\n","  cap = cv2.VideoCapture(path_to_test)\n","\n","  frameCount = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","  frameWidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","  frameHeight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","\n","  video = np.empty((frameCount, frameHeight, frameWidth, 3), np.dtype('uint8'))\n","\n","  fc = 0\n","  ret = True\n","  # Reading each frame into video array + converting to RGB\n","  while fc < frameCount and ret:\n","      ret, video[fc] = cap.read()\n","      video[fc] = cv2.cvtColor(video[fc], cv2.COLOR_BGR2RGB)\n","      fc += 1\n","\n","  cap.release()\n","  # Renders animation in HTML format\n","  rc('animation', html='jshtml', embed_limit=50)\n","\n","  fig, ax = plt.subplots(figsize=(5, 3)) # Using a small figure size\n","                                         # to avoid rendering warnings\n","\n","  # Next we initialize the face detection function\n","  # This was taken from lab 8 [2]\n","  mtcnn = MTCNN()\n","\n","  # Processing 1 frame every 10 and detecting a face accordingly\n","  sampled_indices = list(range(0, frameCount, 10))\n","  # Initalise list to store detectors\n","  faces_per_frame = []\n","\n","  # Run face detection on every sampled frame\n","  for idx in sampled_indices:\n","    faces = mtcnn.detect_faces(video[idx])\n","    faces_per_frame.append(faces)\n","\n","  # Setting the model in evaluation mode\n","  model_ft.eval()\n","\n","  # Preprocessing the images\n","  # These values and image tranforms were taken from lab 8 [2]\n","  data_means = [0.485, 0.456, 0.406]\n","  data_stds = [0.229, 0.224, 0.225]\n","  transform = transforms.Compose([transforms.Resize(256),\n","                      transforms.CenterCrop(224),\n","                      transforms.ToTensor(),\n","                      transforms.Normalize(data_means, data_stds)\n","                      ])\n","\n","  # Setting up the animation function\n","  # The following was taken from lab 4 [1]\n","  def frame(i):\n","    idx = sampled_indices[i]\n","    ax.clear()\n","    ax.axis('off')\n","    fig.tight_layout()\n","    ax.imshow(video[idx])\n","\n","    # Retrieve faces detected for the current frame\n","    faces = faces_per_frame[i]\n","    if faces is not None:\n","      for face in faces:\n","        # Extracting bounding box coordinates\n","        x, y, w, h = face[\"box\"]\n","        # Cropping in order to isolate the face area and feed that in the CNN\n","        cropped_face = video[idx][y:y+h, x:x+h]\n","        # Transforming cropped_face from numpy array to PIL Image\n","        cropped_face = Image.fromarray(cropped_face)\n","\n","        # Applying transformations\n","        face_input = transform(cropped_face)\n","        face_input = face_input.unsqueeze(0) # CNN expects a batch -> [1,C,H,W]\n","        # Setting it to GPU device\n","        face_input = face_input.to(device)\n","\n","        # Predicting the labels\n","        with torch.no_grad():\n","          prediction = model_ft(face_input)\n","          # Finding class with highest score\n","          predicted_class = prediction.argmax(dim=1).item()\n","\n","        # Defining labels\n","        labels = {0:'No Mask', 1:'Correct Mask', 2:'Incorrect Mask'}\n","        label = labels[predicted_class]\n","\n","        # Drawing bounding box\n","        ax.add_patch(patches.Rectangle(xy=(face[\"box\"][0], face[\"box\"][1]),\n","                                       width=face[\"box\"][2],\n","                                       height=face[\"box\"][3],\n","                                       fill=False,\n","                                       color='r',\n","                                       linewidth=2\n","                                       ))\n","        # Adding label text\n","        ax.text(x, y-10, label, color='r', fontsize=10, weight='bold')\n","\n","    return ax\n","\n","\n","  # Animation - from lab 4 [1]\n","  anim = animation.FuncAnimation(fig, frame, frames=len(sampled_indices))\n","  plt.close()\n","  return anim"],"metadata":{"id":"uwq_LEB_smct","executionInfo":{"status":"ok","timestamp":1745763762396,"user_tz":-60,"elapsed":9,"user":{"displayName":"Nicolo Finzi","userId":"07961935095970494315"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["Then, make a call to the `MaskDetection` function to see what results it produces."],"metadata":{"id":"mHVBUe82pLz6"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":398,"output_embedded_package_id":"1icAIzhndfak28CRXB-SWXvOyknUxkOY-"},"id":"2k928S7snTOX","executionInfo":{"status":"ok","timestamp":1745763849149,"user_tz":-60,"elapsed":85433,"user":{"displayName":"Nicolo Finzi","userId":"07961935095970494315"}},"outputId":"7731cd71-2b2a-455f-8925-e61cb9935e66"},"source":["# Syntax for the next function is the following:\n","# MaskDetection(path_to_test)\n","\n","path_to_test = os.path.join(GOOGLE_DRIVE_PATH, 'Personal_Video/mask_video.mp4')\n","anim = MaskDetection(path_to_test)\n","anim"],"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["## References"],"metadata":{"id":"cOuA5aAUnaPY"}},{"cell_type":"markdown","source":["[1] G. Tarroni, *Lab_04,* unpublished lab tutorial, Dept. of Science and Technology, City St. George's, University of London, 2025."],"metadata":{"id":"zUGRQZh_oOql"}},{"cell_type":"markdown","source":["[2] G. Tarroni, *Lab_08_Solved,* unpublished lab tutorial, Dept. of Science and Technology, City St. George's, University of London, 2025."],"metadata":{"id":"89KPqKiIoVxk"}},{"cell_type":"code","source":[],"metadata":{"id":"huokRZzloSvx"},"execution_count":null,"outputs":[]}]}